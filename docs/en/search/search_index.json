{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview A code plugin for Unreal Engine 4 to use ONNX model. Provides easy-to-use functions for accelerated machine learning inference callable from BP and C++ using ONNX Runtime native library. Also, provides utility functions such as resizing, cropping, rotating UTexture and converting it to int8 array. Demo project of human pose estimation and facial capture using a single RGB camera is available on GitHub.","title":"Overview"},{"location":"#overview","text":"A code plugin for Unreal Engine 4 to use ONNX model. Provides easy-to-use functions for accelerated machine learning inference callable from BP and C++ using ONNX Runtime native library. Also, provides utility functions such as resizing, cropping, rotating UTexture and converting it to int8 array. Demo project of human pose estimation and facial capture using a single RGB camera is available on GitHub.","title":"Overview"},{"location":"changelog/","text":"Changelog v1.2 (Feb 18, 2022) Updated TextureProcessing module Added a component to convert UTexture to float array . ( TextureProcessFloatComponent ) Added functions to create UTexture from arrays of byte or float . Fixed a bug that some UTexture cannot be processed by TextureProcessComponent . Now BP_TextureProcessComponent is deprecated. Use TextureProcessComponent instead. Updated CustomizedOpenCV module Removed OpenCV's check function to avoid conflict with UE4's check macro. Added example projects Added an example for depth estimation using a monocular RGB camera . Added an example for arbitrary artistic style transfer . v1.1 (Feb 11, 2022) Added support for Ubuntu 18.04.6 Desktop 64bit GPU accelerations by CUDA and TensorRT supported. You need an NVIDIA GPU which supports CUDA, cuDNN, and TensorRT. You need to install CUDA ver 11.4.2, cuDNN ver 8.2.4, and TensorRT ver 8.2.3.0. DNN models which contain unsupported operators cannot be loaded when TensorRT is enabled. See the official document for supported operators. (NNEngine uses TensorRT 8.2 as backend on Linux) Tested environment: Unreal Engine: 4.26.2, 4.27.2 Vulkan utils: 1.1.70+dfsg1-1ubuntu0.18.04.1 .NET SDK: 6.0.101-1 OS: Ubuntu 18.04.6 Desktop 64bit CPU: Intel i3-8350K GPU: NVIDIA GeForce GTX 1080 Ti Driver: 470.130.01 CUDA: 11.4.2-1 cuDNN: 8.2.4 TensorRT: 8.2.3.0 Added EXPERIMENTAL support for Android as target build Tested environment: Device: Xiaomi Redmi Note 9S Android version: 10 QKQ1.191215.002 Note: You need to convert your model to ORT format. See the official document for the details. There are some DNN models which cannot be loaded on Android. NNEngine uses ONNX Runtime Mobile ver 1.8.1 on Android. GPU acceleration by NNAPI is not tested yet. v1.0 (Dec 21, 2021) First release.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v12-feb-18-2022","text":"Updated TextureProcessing module Added a component to convert UTexture to float array . ( TextureProcessFloatComponent ) Added functions to create UTexture from arrays of byte or float . Fixed a bug that some UTexture cannot be processed by TextureProcessComponent . Now BP_TextureProcessComponent is deprecated. Use TextureProcessComponent instead. Updated CustomizedOpenCV module Removed OpenCV's check function to avoid conflict with UE4's check macro. Added example projects Added an example for depth estimation using a monocular RGB camera . Added an example for arbitrary artistic style transfer .","title":"v1.2 (Feb 18, 2022)"},{"location":"changelog/#v11-feb-11-2022","text":"Added support for Ubuntu 18.04.6 Desktop 64bit GPU accelerations by CUDA and TensorRT supported. You need an NVIDIA GPU which supports CUDA, cuDNN, and TensorRT. You need to install CUDA ver 11.4.2, cuDNN ver 8.2.4, and TensorRT ver 8.2.3.0. DNN models which contain unsupported operators cannot be loaded when TensorRT is enabled. See the official document for supported operators. (NNEngine uses TensorRT 8.2 as backend on Linux) Tested environment: Unreal Engine: 4.26.2, 4.27.2 Vulkan utils: 1.1.70+dfsg1-1ubuntu0.18.04.1 .NET SDK: 6.0.101-1 OS: Ubuntu 18.04.6 Desktop 64bit CPU: Intel i3-8350K GPU: NVIDIA GeForce GTX 1080 Ti Driver: 470.130.01 CUDA: 11.4.2-1 cuDNN: 8.2.4 TensorRT: 8.2.3.0 Added EXPERIMENTAL support for Android as target build Tested environment: Device: Xiaomi Redmi Note 9S Android version: 10 QKQ1.191215.002 Note: You need to convert your model to ORT format. See the official document for the details. There are some DNN models which cannot be loaded on Android. NNEngine uses ONNX Runtime Mobile ver 1.8.1 on Android. GPU acceleration by NNAPI is not tested yet.","title":"v1.1 (Feb 11, 2022)"},{"location":"changelog/#v10-dec-21-2021","text":"First release.","title":"v1.0 (Dec 21, 2021)"},{"location":"demo-project-overview-depth-estimation/","text":"Monocular Depth estimation Example project of depth estimation using a single RGB camera. Environment OS: Windows 10 64bit Unreal Engine: 4.26.2 NNEngine plugin v1.2 or above Download Demo project is available on GitHub . Please download from the release page. Run the demo Extract the downloaded zip file and double-click MonoDepthEstimation.uproject . After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, specify the webcam you want to use in /Content/Common/MediaPlayer_webcam.uasset . Large model If you want to use the large model (whose input image size is 384x384), download from here (GitHub Release page) or here (Google Drive) and place it under Source\\ThirdParty\\Models . Display camera image to the preview mesh By switching the base color pin of /Content/DepthEstimation/Grayscale_WPO.uasset , you can display camera image to the preview mesh. Model details See the following pages for the details of the model used in this project. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer GitHub","title":"Monocular Depth estimation"},{"location":"demo-project-overview-depth-estimation/#monocular-depth-estimation","text":"Example project of depth estimation using a single RGB camera.","title":"Monocular Depth estimation"},{"location":"demo-project-overview-depth-estimation/#environment","text":"OS: Windows 10 64bit Unreal Engine: 4.26.2 NNEngine plugin v1.2 or above","title":"Environment"},{"location":"demo-project-overview-depth-estimation/#download","text":"Demo project is available on GitHub . Please download from the release page.","title":"Download"},{"location":"demo-project-overview-depth-estimation/#run-the-demo","text":"Extract the downloaded zip file and double-click MonoDepthEstimation.uproject . After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, specify the webcam you want to use in /Content/Common/MediaPlayer_webcam.uasset .","title":"Run the demo"},{"location":"demo-project-overview-depth-estimation/#large-model","text":"If you want to use the large model (whose input image size is 384x384), download from here (GitHub Release page) or here (Google Drive) and place it under Source\\ThirdParty\\Models .","title":"Large model"},{"location":"demo-project-overview-depth-estimation/#display-camera-image-to-the-preview-mesh","text":"By switching the base color pin of /Content/DepthEstimation/Grayscale_WPO.uasset , you can display camera image to the preview mesh.","title":"Display camera image to the preview mesh"},{"location":"demo-project-overview-depth-estimation/#model-details","text":"See the following pages for the details of the model used in this project. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer GitHub","title":"Model details"},{"location":"demo-project-overview-style-transfer/","text":"Artistic Style Transfer Example project of artistic style transfer, where a new image is created based on two inputs, one representing the artistic style and one representing the content. Environment OS: Windows 10 64bit Unreal Engine: 4.26.2 NNEngine plugin v1.2 or above Download Demo project is available on GitHub . Please download from the release page. Run the demo Extract the downloaded zip file and double-click ArtStyleTransfer.uproject . After launching, follow the tutorial of /Content/NNEngineDemo-ArtisticStyleTransfer/Tutorial_ArtisticStyleTransfer.uasset . Add an artistic Style You can add a style by dragging and dropping any image file into the UE4 content browser to create a Texture asset, and then specifying it as the Style Texture of the ArtisticStyleTransferer actor. Model details See the following pages for the details of the model used in this project. Exploring the structure of a real-time, arbitrary neural artistic stylization network TFLite Tutorial","title":"Artistic Style Transfer"},{"location":"demo-project-overview-style-transfer/#artistic-style-transfer","text":"Example project of artistic style transfer, where a new image is created based on two inputs, one representing the artistic style and one representing the content.","title":"Artistic Style Transfer"},{"location":"demo-project-overview-style-transfer/#environment","text":"OS: Windows 10 64bit Unreal Engine: 4.26.2 NNEngine plugin v1.2 or above","title":"Environment"},{"location":"demo-project-overview-style-transfer/#download","text":"Demo project is available on GitHub . Please download from the release page.","title":"Download"},{"location":"demo-project-overview-style-transfer/#run-the-demo","text":"Extract the downloaded zip file and double-click ArtStyleTransfer.uproject . After launching, follow the tutorial of /Content/NNEngineDemo-ArtisticStyleTransfer/Tutorial_ArtisticStyleTransfer.uasset .","title":"Run the demo"},{"location":"demo-project-overview-style-transfer/#add-an-artistic-style","text":"You can add a style by dragging and dropping any image file into the UE4 content browser to create a Texture asset, and then specifying it as the Style Texture of the ArtisticStyleTransferer actor.","title":"Add an artistic Style"},{"location":"demo-project-overview-style-transfer/#model-details","text":"See the following pages for the details of the model used in this project. Exploring the structure of a real-time, arbitrary neural artistic stylization network TFLite Tutorial","title":"Model details"},{"location":"demo-project-overview/","text":"Human pose estimation Example project of human pose estimation, facial capture and eye tracking using a single RGB camera. Environment OS: Windows 10 64bit or Linux x64 (Ubuntu 18.04.6) Unreal Engine: 4.26 NNEngine plugin For Windows Visual Studio is required to be installed. See the official documentation for how to install. For Linux Visual Studio Code is recommended for IDE. See this guide for how to install. Download Demo project is available on GitHub . Please download from the release page. Run the demo Windows Linux Extract the downloaded zip file and double-click NNEngineDemo.uproject . The first time you start it, you need to build the C++ source. After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, follow the instructions in Content\\NNEngineDemo\\Tutorial.uasset and specify the webcam you want to use in WebcamMediaPlayer's Video. Extract the downloaded zip file Create a directory in the extracted directory and name it Plugins , and copy the NNEngine plugin into it. Double-click NNEngineDemo.uproject . In the pop-up, select Unreal Engine 4.26 . You will be asked to rebuild. Select No . Double-click the generated NNEngineDemo.code-workspace to start VS Code. In VS Code, select NNEngineDemoEditor (Development) (NNEngineDemo) from Run and Debug , and click Start Debugging. After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, follow the instructions in Content\\NNEngineDemo\\Tutorial.uasset and specify the webcam you want to use in WebcamMediaPlayer's Video. Classes in the demo The tutorial below explains the relationships between the classes in each folder. Content\\NNEngineDemo\\FacialCapture\\Overview_FacialCapture.uasset Content\\NNEngineDemo\\MotionCapture_Bp\\Overview_PoseEstimation.uasset Content\\NNEngineDemo\\MotionCapture_Cpp\\Overview_PoseEstimation_Cpp.uasset Implementation of the demo The tutorial below explains the implementation. Content\\NNEngineDemo\\MotionCapture_Bp\\Details_PoseEstimation.uasset","title":"Human pose estimation"},{"location":"demo-project-overview/#human-pose-estimation","text":"Example project of human pose estimation, facial capture and eye tracking using a single RGB camera.","title":"Human pose estimation"},{"location":"demo-project-overview/#environment","text":"OS: Windows 10 64bit or Linux x64 (Ubuntu 18.04.6) Unreal Engine: 4.26 NNEngine plugin For Windows Visual Studio is required to be installed. See the official documentation for how to install. For Linux Visual Studio Code is recommended for IDE. See this guide for how to install.","title":"Environment"},{"location":"demo-project-overview/#download","text":"Demo project is available on GitHub . Please download from the release page.","title":"Download"},{"location":"demo-project-overview/#run-the-demo","text":"Windows Linux Extract the downloaded zip file and double-click NNEngineDemo.uproject . The first time you start it, you need to build the C++ source. After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, follow the instructions in Content\\NNEngineDemo\\Tutorial.uasset and specify the webcam you want to use in WebcamMediaPlayer's Video. Extract the downloaded zip file Create a directory in the extracted directory and name it Plugins , and copy the NNEngine plugin into it. Double-click NNEngineDemo.uproject . In the pop-up, select Unreal Engine 4.26 . You will be asked to rebuild. Select No . Double-click the generated NNEngineDemo.code-workspace to start VS Code. In VS Code, select NNEngineDemoEditor (Development) (NNEngineDemo) from Run and Debug , and click Start Debugging. After launching, click Play on the editor to start the demo that performs AI estimation for the pre-recorded video. To run on your webcam, follow the instructions in Content\\NNEngineDemo\\Tutorial.uasset and specify the webcam you want to use in WebcamMediaPlayer's Video.","title":"Run the demo"},{"location":"demo-project-overview/#classes-in-the-demo","text":"The tutorial below explains the relationships between the classes in each folder. Content\\NNEngineDemo\\FacialCapture\\Overview_FacialCapture.uasset Content\\NNEngineDemo\\MotionCapture_Bp\\Overview_PoseEstimation.uasset Content\\NNEngineDemo\\MotionCapture_Cpp\\Overview_PoseEstimation_Cpp.uasset","title":"Classes in the demo"},{"location":"demo-project-overview/#implementation-of-the-demo","text":"The tutorial below explains the implementation. Content\\NNEngineDemo\\MotionCapture_Bp\\Details_PoseEstimation.uasset","title":"Implementation of the demo"},{"location":"how-to-use-customized-opencv-module/","text":"How to use CustomizedOpenCV module Based on OpenCV 4.4.0, some functions are disabled so that it can be built with Unreal Engine. It can be used in C++ by including the OpenCV header as shown below. #undef check // the check macro causes problems with opencv headers #include \"opencv2/core/core.hpp\" Note that this module is created only for use from the TextureProcessing module, and not all OpenCV functions can be used. See the official documentation for how to use OpenCV itself.","title":"CustomizedOpenCV module"},{"location":"how-to-use-customized-opencv-module/#how-to-use-customizedopencv-module","text":"Based on OpenCV 4.4.0, some functions are disabled so that it can be built with Unreal Engine. It can be used in C++ by including the OpenCV header as shown below. #undef check // the check macro causes problems with opencv headers #include \"opencv2/core/core.hpp\" Note that this module is created only for use from the TextureProcessing module, and not all OpenCV functions can be used. See the official documentation for how to use OpenCV itself.","title":"How to use CustomizedOpenCV module"},{"location":"how-to-use-directx-utility-module/","text":"How to use DirectXUtility module Works on Windows only. An example can be found in \"Content\\NNEngineDemo\\Common\\WidgetToChangeCpuGpu.uasset\" of the demo project. Call the \"Get Gpu Info\" node to get an array of Gpu Info structures, which is a list of GPUs available on your PC. You can get the device ID and name from the Gpu Info structure. You can specify which GPU to use by specifying this device ID when initializing the UOnnxModelWrapper or OnnxModel.","title":"DirectXUtility module"},{"location":"how-to-use-directx-utility-module/#how-to-use-directxutility-module","text":"Works on Windows only. An example can be found in \"Content\\NNEngineDemo\\Common\\WidgetToChangeCpuGpu.uasset\" of the demo project. Call the \"Get Gpu Info\" node to get an array of Gpu Info structures, which is a list of GPUs available on your PC. You can get the device ID and name from the Gpu Info structure. You can specify which GPU to use by specifying this device ID when initializing the UOnnxModelWrapper or OnnxModel.","title":"How to use DirectXUtility module"},{"location":"how-to-use-onnxruntime-module/","text":"How to use OnnxRuntime module This module wraps the ONNX Runtime's C++ API and makes it easy to call from Unreal Engine's Blueprints and C++. An example of use from BP can be found in Plugins\\NNEngine\\Content\\MinimalExample\\MinimalExampleOfOnnxModelWrapper.uasset . ONNX? See Overview of ONNX for more information on what ONNX is. ONNX Runtime? See the official documentation for how to use ONNX Runtime itself. Load ONNX model Load the AI \u200b\u200bmodel saved in ONNX format. NNEngine reads the .onnx file by specifying the path at runtime. BP C++ Create a Blueprint class and add a variable of UOnnxModelWrapper. Construct UOnnxModelWrapper and call \"Init\". Specify the path to the ONNX model Specify whether to use CPU or GPU, and which GPU to use. To get available GPUs on the system, call \"Get Gpu Info\". Create a C++ class and add a variable of OnnxModel. Call the constructor of OnnxModel. Specify the path to the ONNX model Specify whether to use CPU or GPU, and which GPU to use. To get available GPUs on the system, call UDirectXUtilityLibrary::GetGpuInfo() . #pragma once #include \"OnnxModel.h\" #include \"OnnxModelMinimumExample.generated.h\" UCLASS(Blueprintable, Category = \"ONNX Runtime\") class ONNXRUNTIME_API UOnnxModelMinimumExample : public UObject { GENERATED_BODY() protected: OnnxModel* onnxModel; public: UOnnxModelMinimumExample() { onnxModel = new OnnxModel(\"Full-path-to-your-AI.onnx\", EOnnxProvider::GPU_DirectML, 0); } }; Specify input source BP C++ Call \"Get Input Tensor Info\" to confirm the order of the input tensors as well as their types and sizes. Add variables of byte, integer, integer64, or float arrays whose types and sizes match the previous results. Call \"Bind Input xxx Array\" for each input tensor and specify the created array as the data input sources to the ONNX model. Get \"inputTensorsInfo\" member to confirm the order of the input tensors as well as their types and sizes. Add variables of arrays whose size in bytes match the previous results. Call \"bindInput\" as many as the number of input tensors and specify the created arrays as the data input sources to the ONNX model. TArray<uint8> inputDataBuffer0; TArray<uint8> inputDataBuffer1; void setupInputs() { inputDataBuffer0.Init(0, 1 * 256 * 256 * 3); inputDataBuffer1.Init(0, 1); onnxModel->bindInput(onnxModel->inputTensorsInfo[0], inputDataBuffer0.GetData()); onnxModel->bindInput(onnxModel->inputTensorsInfo[1], inputDataBuffer1.GetData()); } Specify output destination BP C++ Call \"Get Output Tensor Info\" to confirm the order of the output tensors as well as their types and sizes. Add variables of byte, integer, integer64, or float arrays whose types and sizes match the previous results. Call \"Bind Output xxx Array\" for each output tensor and specify the created array as the data output destination from the ONNX model. Get \"outputTensorsInfo\" member to confirm the order of the output tensors as well as their types and sizes. Add variables of array whose size in bytes match the previous result. Call \"bindOutput\" for each output tensor and specify the created array as the data output destination from the ONNX model. TArray<uint8> outputDataBuffer0; TArray<uint8> outputDataBuffer1; void setupOutputs() { outputDataBuffer0.Init(0, 17 * 3); outputDataBuffer1.Init(0, 4 * 4); onnxModel->bindOutput(onnxModel->outputTensorsInfo[0], outputDataBuffer0.GetData()); onnxModel->bindOutput(onnxModel->outputTensorsInfo[1], outputDataBuffer1.GetData()); } Run Set data to the array specified as the data input source for the ONNX model. Call \"Run\". Get data from the array specified as the data output destination for the ONNX model. BP C++ onnxModel->run();","title":"OnnxRuntime module"},{"location":"how-to-use-onnxruntime-module/#how-to-use-onnxruntime-module","text":"This module wraps the ONNX Runtime's C++ API and makes it easy to call from Unreal Engine's Blueprints and C++. An example of use from BP can be found in Plugins\\NNEngine\\Content\\MinimalExample\\MinimalExampleOfOnnxModelWrapper.uasset . ONNX? See Overview of ONNX for more information on what ONNX is. ONNX Runtime? See the official documentation for how to use ONNX Runtime itself.","title":"How to use OnnxRuntime module"},{"location":"how-to-use-onnxruntime-module/#load-onnx-model","text":"Load the AI \u200b\u200bmodel saved in ONNX format. NNEngine reads the .onnx file by specifying the path at runtime. BP C++ Create a Blueprint class and add a variable of UOnnxModelWrapper. Construct UOnnxModelWrapper and call \"Init\". Specify the path to the ONNX model Specify whether to use CPU or GPU, and which GPU to use. To get available GPUs on the system, call \"Get Gpu Info\". Create a C++ class and add a variable of OnnxModel. Call the constructor of OnnxModel. Specify the path to the ONNX model Specify whether to use CPU or GPU, and which GPU to use. To get available GPUs on the system, call UDirectXUtilityLibrary::GetGpuInfo() . #pragma once #include \"OnnxModel.h\" #include \"OnnxModelMinimumExample.generated.h\" UCLASS(Blueprintable, Category = \"ONNX Runtime\") class ONNXRUNTIME_API UOnnxModelMinimumExample : public UObject { GENERATED_BODY() protected: OnnxModel* onnxModel; public: UOnnxModelMinimumExample() { onnxModel = new OnnxModel(\"Full-path-to-your-AI.onnx\", EOnnxProvider::GPU_DirectML, 0); } };","title":"Load ONNX model"},{"location":"how-to-use-onnxruntime-module/#specify-input-source","text":"BP C++ Call \"Get Input Tensor Info\" to confirm the order of the input tensors as well as their types and sizes. Add variables of byte, integer, integer64, or float arrays whose types and sizes match the previous results. Call \"Bind Input xxx Array\" for each input tensor and specify the created array as the data input sources to the ONNX model. Get \"inputTensorsInfo\" member to confirm the order of the input tensors as well as their types and sizes. Add variables of arrays whose size in bytes match the previous results. Call \"bindInput\" as many as the number of input tensors and specify the created arrays as the data input sources to the ONNX model. TArray<uint8> inputDataBuffer0; TArray<uint8> inputDataBuffer1; void setupInputs() { inputDataBuffer0.Init(0, 1 * 256 * 256 * 3); inputDataBuffer1.Init(0, 1); onnxModel->bindInput(onnxModel->inputTensorsInfo[0], inputDataBuffer0.GetData()); onnxModel->bindInput(onnxModel->inputTensorsInfo[1], inputDataBuffer1.GetData()); }","title":"Specify input source"},{"location":"how-to-use-onnxruntime-module/#specify-output-destination","text":"BP C++ Call \"Get Output Tensor Info\" to confirm the order of the output tensors as well as their types and sizes. Add variables of byte, integer, integer64, or float arrays whose types and sizes match the previous results. Call \"Bind Output xxx Array\" for each output tensor and specify the created array as the data output destination from the ONNX model. Get \"outputTensorsInfo\" member to confirm the order of the output tensors as well as their types and sizes. Add variables of array whose size in bytes match the previous result. Call \"bindOutput\" for each output tensor and specify the created array as the data output destination from the ONNX model. TArray<uint8> outputDataBuffer0; TArray<uint8> outputDataBuffer1; void setupOutputs() { outputDataBuffer0.Init(0, 17 * 3); outputDataBuffer1.Init(0, 4 * 4); onnxModel->bindOutput(onnxModel->outputTensorsInfo[0], outputDataBuffer0.GetData()); onnxModel->bindOutput(onnxModel->outputTensorsInfo[1], outputDataBuffer1.GetData()); }","title":"Specify output destination"},{"location":"how-to-use-onnxruntime-module/#run","text":"Set data to the array specified as the data input source for the ONNX model. Call \"Run\". Get data from the array specified as the data output destination for the ONNX model. BP C++ onnxModel->run();","title":"Run"},{"location":"how-to-use-texture-processing-module/","text":"How to use TextureProcessing module Use TextureProcessComponent to precess images. TextureProcessComponent is a component for converting a UTexture image into a byte array of a specified size, as well as scaling, cropping, and rotating the image. The resulting byte array can be used as input data to AI. An example can be found in Content\\NNEngineDemo\\MotionCapture_Bp\\MotionCapture_BpImplementation.uasset of the demo project. Create component Create a Blueprint class and add TextureProcessComponent . Specify the image size after image processing as the initial values \u200b\u200bof Destination Width and Destination Height. This TextureProcessComponent will output the result to a byte array with a size of (Destination Height x Destination Width x 3). To use float array Use TextureProcessFloatComponent which outputs results into a float array. Simple Scaling You can scale an image to the size specified at initialization by calling the Resize node of TextureProcessComponent . Input Input texture : Original image. Do Flip Image : Whether to flip the image during resizing. Do Rotate Image : Whether to rotate the image during resizing. Output outputHxWxBGR : An array of BGR values of each pixel of the image resized to (Destination Height x Destination Width), while keeping the aspect ratio. If the input and output images have different aspect ratios, part of the output array will be filled with some values. uvScalingFactor : The ratio of the aspect ratios of input and output images. By multiplying the UV coordinates in the output image by this value, you can get the UV coordinates in the input image. For example, if the input image is 16:9 and the output image is square, the uvScalingFactor will be (1, 1.777\u2026). Affine transform By calling the Affine Transform node of TextureProcessComponent , you can transform the input image and then scale it to the size specified at initialization. See a linear algebra textbook for affine transformation itself. Input Input texture : Original image. Inverse Normalized Affine Mat : Specifies the inverse of the matrix which represents the affine transformation when the input / output image size is (1, 1). This is equal to the matrix which represents the affine transformation in UV coordinates. Do Flip Image : Whether to flip the image during resizing. Do Rotate Image : Whether to rotate the image during resizing. Output outputHxWxBGR : An array of BGR values of each pixel of the image after transformation and resizing to (Destination Height x Destination Width). Function to find the Affine transformation matrix 1 By calling the Get Inverse Affine Mat node, you can find the inverse matrix of the matrix that represents the affine transformation that crops an image to an arbitrary square area. Input Center : The center of the square area in the coordinate of the input image. Orientation : The orientation of the Y-axis of the square area in the coordinate of the input image. For example, (0, 1) when there is no rotation, and (-1, 0) when rotating 90 degrees clockwise. Size : The length of the side of the square area in the coordinate of the input image. Output Out Inverse Affine Mat : Inverse matrix of the matrix representing the obtained affine transformation Function to find the Affine transformation matrix 2 By calling the Get Inverse Affine Mat From 2 Points node, you can find the inverse matrix of the matrix that represents the affine transformation that crops an image to an arbitrary square area. Input Center : The center of the square area in the coordinate of the input image. Top Center : The center of the top edge of the square area in the coordinate of the input image. Scaling Factor : Parameter for adjusting the size of the square area. The side of the square area will be (The distance between Center and Top Center * 2 * this value). Output Out Inverse Affine Mat : Inverse matrix of the matrix representing the obtained affine transformation Create Texture from an array of bytes or floats Create Texture2D objects Call CreateTexture2d_xxx_yyy nodes to create Texture2D objects. CreateTexture2d_Gray_Byte : Create Texture2D with one color channel. Each pixel has a uint8 value. CreateTexture2d_Gray_Float : Create Texture2D with one color channel. Each pixel has a float32 value. CreateTexture2d_BGRA_Byte : Create Texture2D with 4(BGRA) color channel. Each pixel has 4 uint8 values. CreateTexture2d_RGBA_Float : Create Texture2D with 4(RGBA) color channel. Each pixel has 4 float32 values. Copy data from an array to Texture2D objects Call CopyByteArrayToTexture2D_zzz nodes to copy data from the array to Texture2D. Warning The size of the input array must be equal to the number of the pixel of output texture. CopyByteArrayToTexture2D : Copy the data of a byte array to a Texture2D object. CopyFloatArrayToTexture2D : Copy the data of a float array to a Texture2D object. CopyByteArrayToTexture2D_RGB_To_BGRA : Copy the data of a byte array to a Texture2D object while converting RGB format to BGRA format. CopyFloatArrayToTexture2D_RGB_To_RGBA : Copy the data of a float array to a Texture2D object while converting RGB format to RGBA format.","title":"TextureProcessing module"},{"location":"how-to-use-texture-processing-module/#how-to-use-textureprocessing-module","text":"Use TextureProcessComponent to precess images. TextureProcessComponent is a component for converting a UTexture image into a byte array of a specified size, as well as scaling, cropping, and rotating the image. The resulting byte array can be used as input data to AI. An example can be found in Content\\NNEngineDemo\\MotionCapture_Bp\\MotionCapture_BpImplementation.uasset of the demo project.","title":"How to use TextureProcessing module"},{"location":"how-to-use-texture-processing-module/#create-component","text":"Create a Blueprint class and add TextureProcessComponent . Specify the image size after image processing as the initial values \u200b\u200bof Destination Width and Destination Height. This TextureProcessComponent will output the result to a byte array with a size of (Destination Height x Destination Width x 3). To use float array Use TextureProcessFloatComponent which outputs results into a float array.","title":"Create component"},{"location":"how-to-use-texture-processing-module/#simple-scaling","text":"You can scale an image to the size specified at initialization by calling the Resize node of TextureProcessComponent .","title":"Simple Scaling"},{"location":"how-to-use-texture-processing-module/#input","text":"Input texture : Original image. Do Flip Image : Whether to flip the image during resizing. Do Rotate Image : Whether to rotate the image during resizing.","title":"Input"},{"location":"how-to-use-texture-processing-module/#output","text":"outputHxWxBGR : An array of BGR values of each pixel of the image resized to (Destination Height x Destination Width), while keeping the aspect ratio. If the input and output images have different aspect ratios, part of the output array will be filled with some values. uvScalingFactor : The ratio of the aspect ratios of input and output images. By multiplying the UV coordinates in the output image by this value, you can get the UV coordinates in the input image. For example, if the input image is 16:9 and the output image is square, the uvScalingFactor will be (1, 1.777\u2026).","title":"Output"},{"location":"how-to-use-texture-processing-module/#affine-transform","text":"By calling the Affine Transform node of TextureProcessComponent , you can transform the input image and then scale it to the size specified at initialization. See a linear algebra textbook for affine transformation itself.","title":"Affine transform"},{"location":"how-to-use-texture-processing-module/#input_1","text":"Input texture : Original image. Inverse Normalized Affine Mat : Specifies the inverse of the matrix which represents the affine transformation when the input / output image size is (1, 1). This is equal to the matrix which represents the affine transformation in UV coordinates. Do Flip Image : Whether to flip the image during resizing. Do Rotate Image : Whether to rotate the image during resizing.","title":"Input"},{"location":"how-to-use-texture-processing-module/#output_1","text":"outputHxWxBGR : An array of BGR values of each pixel of the image after transformation and resizing to (Destination Height x Destination Width).","title":"Output"},{"location":"how-to-use-texture-processing-module/#function-to-find-the-affine-transformation-matrix-1","text":"By calling the Get Inverse Affine Mat node, you can find the inverse matrix of the matrix that represents the affine transformation that crops an image to an arbitrary square area.","title":"Function to find the Affine transformation matrix 1"},{"location":"how-to-use-texture-processing-module/#input_2","text":"Center : The center of the square area in the coordinate of the input image. Orientation : The orientation of the Y-axis of the square area in the coordinate of the input image. For example, (0, 1) when there is no rotation, and (-1, 0) when rotating 90 degrees clockwise. Size : The length of the side of the square area in the coordinate of the input image.","title":"Input"},{"location":"how-to-use-texture-processing-module/#output_2","text":"Out Inverse Affine Mat : Inverse matrix of the matrix representing the obtained affine transformation","title":"Output"},{"location":"how-to-use-texture-processing-module/#function-to-find-the-affine-transformation-matrix-2","text":"By calling the Get Inverse Affine Mat From 2 Points node, you can find the inverse matrix of the matrix that represents the affine transformation that crops an image to an arbitrary square area.","title":"Function to find the Affine transformation matrix 2"},{"location":"how-to-use-texture-processing-module/#input_3","text":"Center : The center of the square area in the coordinate of the input image. Top Center : The center of the top edge of the square area in the coordinate of the input image. Scaling Factor : Parameter for adjusting the size of the square area. The side of the square area will be (The distance between Center and Top Center * 2 * this value).","title":"Input"},{"location":"how-to-use-texture-processing-module/#output_3","text":"Out Inverse Affine Mat : Inverse matrix of the matrix representing the obtained affine transformation","title":"Output"},{"location":"how-to-use-texture-processing-module/#create-texture-from-an-array-of-bytes-or-floats","text":"","title":"Create Texture from an array of bytes or floats"},{"location":"how-to-use-texture-processing-module/#create-texture2d-objects","text":"Call CreateTexture2d_xxx_yyy nodes to create Texture2D objects. CreateTexture2d_Gray_Byte : Create Texture2D with one color channel. Each pixel has a uint8 value. CreateTexture2d_Gray_Float : Create Texture2D with one color channel. Each pixel has a float32 value. CreateTexture2d_BGRA_Byte : Create Texture2D with 4(BGRA) color channel. Each pixel has 4 uint8 values. CreateTexture2d_RGBA_Float : Create Texture2D with 4(RGBA) color channel. Each pixel has 4 float32 values.","title":"Create Texture2D objects"},{"location":"how-to-use-texture-processing-module/#copy-data-from-an-array-to-texture2d-objects","text":"Call CopyByteArrayToTexture2D_zzz nodes to copy data from the array to Texture2D. Warning The size of the input array must be equal to the number of the pixel of output texture. CopyByteArrayToTexture2D : Copy the data of a byte array to a Texture2D object. CopyFloatArrayToTexture2D : Copy the data of a float array to a Texture2D object. CopyByteArrayToTexture2D_RGB_To_BGRA : Copy the data of a byte array to a Texture2D object while converting RGB format to BGRA format. CopyFloatArrayToTexture2D_RGB_To_RGBA : Copy the data of a float array to a Texture2D object while converting RGB format to RGBA format.","title":"Copy data from an array to Texture2D objects"},{"location":"install/","text":"Installation Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open \"Edit > Plugins\" on the editor menu, enable \"NNEngine\", and restart the project. Install to Linux Since the Epic Games Launcher is not provided for Linux, you need to copy the plugin manually from Windows. On Windows, install the plugin from Epic Games Launcher. On Linux, create a project. Copy the plugin from the UE4 plugin folder on Windows to the project directory on Linux. Copy from: < UE4 installation folder on Windows >\\Engine\\Plugins\\Marketplace\\NNEngine Copy to: < directory containing the .uporject created on Linux >/Plugins/NNEngine","title":"Installation"},{"location":"install/#installation","text":"Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open \"Edit > Plugins\" on the editor menu, enable \"NNEngine\", and restart the project. Install to Linux Since the Epic Games Launcher is not provided for Linux, you need to copy the plugin manually from Windows. On Windows, install the plugin from Epic Games Launcher. On Linux, create a project. Copy the plugin from the UE4 plugin folder on Windows to the project directory on Linux. Copy from: < UE4 installation folder on Windows >\\Engine\\Plugins\\Marketplace\\NNEngine Copy to: < directory containing the .uporject created on Linux >/Plugins/NNEngine","title":"Installation"},{"location":"modules/","text":"Modules NNEngine consists of the following four modules. Module Description OnnxRuntime A module for executing AI using ONNX files. If you just want to use ONNX format AI, you only need this module. TextureProcessing A module for performing image processing on UTexture and creating input data to ONNX. It provides functions such as conversion from UTexture to byte array, scaling, cropping, and rotation. DirectXUtility (Windows only) A module for getting a list of GPUs on a Windows PC. It is used to display a list of GPUs on the end user's PC to let the user to select one. CustomizedOpenCV A module that provides a set of useful functions for image processing. It is used by \"2. TextureProcessing\" module. Overview for the OnnxRuntime and TextureProcessing modules:","title":"Modules"},{"location":"modules/#modules","text":"NNEngine consists of the following four modules. Module Description OnnxRuntime A module for executing AI using ONNX files. If you just want to use ONNX format AI, you only need this module. TextureProcessing A module for performing image processing on UTexture and creating input data to ONNX. It provides functions such as conversion from UTexture to byte array, scaling, cropping, and rotation. DirectXUtility (Windows only) A module for getting a list of GPUs on a Windows PC. It is used to display a list of GPUs on the end user's PC to let the user to select one. CustomizedOpenCV A module that provides a set of useful functions for image processing. It is used by \"2. TextureProcessing\" module. Overview for the OnnxRuntime and TextureProcessing modules:","title":"Modules"},{"location":"onnx-introduction/","text":"Overview of ONNX For those who have never used AI in ONNX format, here is a brief overview of ONNX. If you know the basics of ONNX, you don't need to read this. What's ONNX? ONNX = Open Neural Network eXchange A format to use AI in various environment. You can create files in this format from various machine learning frameworks , and you can run AI on various platforms with this format. (https://onnxruntime.ai/docs/execution-providers/ \u3088\u308a\u5f15\u7528) How to get ONNX file You can make it yourself from PyTorch, TensorFlow, etc. Also there are kind people who publish ONNX format AI for free. (Example 1) ONNX Model Zoo : ONNX official (Example 2) PINTO model zoo : A repository where PINTO shares the results of optimization of various models. Various formats are provided, including ONNX. Check the license carefully before using them.\ud83d\ude4f How to use ONNX Roughly there are 4 steps. Load the ONNX file. Specify the input data source. Specify the output data destination. Run. See the Official Documentation for the details. NNEngine provides convenient functions to execute 1 to 4 above.","title":"Overview of ONNX"},{"location":"onnx-introduction/#overview-of-onnx","text":"For those who have never used AI in ONNX format, here is a brief overview of ONNX. If you know the basics of ONNX, you don't need to read this.","title":"Overview of ONNX"},{"location":"onnx-introduction/#whats-onnx","text":"ONNX = Open Neural Network eXchange A format to use AI in various environment. You can create files in this format from various machine learning frameworks , and you can run AI on various platforms with this format. (https://onnxruntime.ai/docs/execution-providers/ \u3088\u308a\u5f15\u7528)","title":"What's ONNX?"},{"location":"onnx-introduction/#how-to-get-onnx-file","text":"You can make it yourself from PyTorch, TensorFlow, etc. Also there are kind people who publish ONNX format AI for free. (Example 1) ONNX Model Zoo : ONNX official (Example 2) PINTO model zoo : A repository where PINTO shares the results of optimization of various models. Various formats are provided, including ONNX. Check the license carefully before using them.\ud83d\ude4f","title":"How to get ONNX file"},{"location":"onnx-introduction/#how-to-use-onnx","text":"Roughly there are 4 steps. Load the ONNX file. Specify the input data source. Specify the output data destination. Run. See the Official Documentation for the details. NNEngine provides convenient functions to execute 1 to 4 above.","title":"How to use ONNX"},{"location":"system-requirement/","text":"System Requirements Supported Unreal Engine version: 4.26 4.27 Supported Platforms Platform Development Target Build Windows 10 64bit \u2705 \u2705 Ubuntu 18.04.6 Desktop 64bit \u2705 \u2705 Android \u2705(Experimental) Model format for Android To use on Android, DNN models need to be converted to ORT format. See the official document for the details. Known issue There are some DNN models which cannot be loaded on Android. Supported hardware acceleration Platform Default CPU GPU DirectML GPU CUDA GPU TensorRT NNAPI Windows 10 64bit \u2705 \u2705 \u2705 \u2705 Ubuntu 18.04.6 Desktop 64bit \u2705 \u2705 \u2705 Android \u2705 (Not tested yet) To use GPU acceleration with DirectML, a DirectX 12 capable GPU is required. To use GPU acceleration with CUDA and TensorRT, a supported NVIDIA GPU is required and the following versions of CUDA, cuDNN, and TensorRT are required to be installed. CUDA, cuDNN, TensorRT versions Windows Linux Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN 8.0.2 (July 24th, 2020), for CUDA 11.0 8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 For RTX2080Ti users If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations. CUDA 11.4.2 for Linux x86_64 Ubuntu 18.04 cuDNN 8.2.4 (September 2nd, 2021), for CUDA 11.4, Linux x86_64 TensorRT 8.2.3.0 (8.2 GA Update 2) for Linux x86_64, CUDA 11.0-11.5 Supported operators for TensorRT On Linux, DNN models which contain unsupported operators cannot be loaded when TensorRT is enabled. See the official document for supported operators. (NNEngine uses TensorRT 8.2 as backend on Linux)","title":"System Requirements"},{"location":"system-requirement/#system-requirements","text":"","title":"System Requirements"},{"location":"system-requirement/#supported-unreal-engine-version","text":"4.26 4.27","title":"Supported Unreal Engine version:"},{"location":"system-requirement/#supported-platforms","text":"Platform Development Target Build Windows 10 64bit \u2705 \u2705 Ubuntu 18.04.6 Desktop 64bit \u2705 \u2705 Android \u2705(Experimental) Model format for Android To use on Android, DNN models need to be converted to ORT format. See the official document for the details. Known issue There are some DNN models which cannot be loaded on Android.","title":"Supported Platforms"},{"location":"system-requirement/#supported-hardware-acceleration","text":"Platform Default CPU GPU DirectML GPU CUDA GPU TensorRT NNAPI Windows 10 64bit \u2705 \u2705 \u2705 \u2705 Ubuntu 18.04.6 Desktop 64bit \u2705 \u2705 \u2705 Android \u2705 (Not tested yet) To use GPU acceleration with DirectML, a DirectX 12 capable GPU is required. To use GPU acceleration with CUDA and TensorRT, a supported NVIDIA GPU is required and the following versions of CUDA, cuDNN, and TensorRT are required to be installed.","title":"Supported hardware acceleration"},{"location":"system-requirement/#cuda-cudnn-tensorrt-versions","text":"Windows Linux Other than RTX30** series RTX30** series CUDA 11.0.3 11.0.3 cuDNN 8.0.2 (July 24th, 2020), for CUDA 11.0 8.0.5 (November 9th, 2020), for CUDA 11.0 TensorRT 7.1.3.4 for CUDA 11.0 7.2.2.3 for CUDA 11.0 For RTX2080Ti users If you are using RTX2080Ti, you might need to use cuDNN v8.0.5. Please try various combinations. CUDA 11.4.2 for Linux x86_64 Ubuntu 18.04 cuDNN 8.2.4 (September 2nd, 2021), for CUDA 11.4, Linux x86_64 TensorRT 8.2.3.0 (8.2 GA Update 2) for Linux x86_64, CUDA 11.0-11.5 Supported operators for TensorRT On Linux, DNN models which contain unsupported operators cannot be loaded when TensorRT is enabled. See the official document for supported operators. (NNEngine uses TensorRT 8.2 as backend on Linux)","title":"CUDA, cuDNN, TensorRT versions"},{"location":"tips-build/","text":"Build without CUDA and TensorRT This plugin contains a large (162MB) DLL, \"onnxruntime_providers_cuda.dll\". You may want to exclude it to reduce packaged game size. To do that, you need to disable CUDA and TensorRT execution providers by following the steps below: Open \"(Your installation path of UE4)/Engine/Plugins/Marketplace/NNEngine/Source/OnnxRuntime/OnnxRuntime.Build.cs\" Change line 20 and 21 as follows: Before bool doUseCuda = true; bool doUseTensorRT = true; After bool doUseCuda = false; bool doUseTensorRT = false; Build the project.","title":"Build without CUDA and TensorRT"},{"location":"tips-build/#build-without-cuda-and-tensorrt","text":"This plugin contains a large (162MB) DLL, \"onnxruntime_providers_cuda.dll\". You may want to exclude it to reduce packaged game size. To do that, you need to disable CUDA and TensorRT execution providers by following the steps below: Open \"(Your installation path of UE4)/Engine/Plugins/Marketplace/NNEngine/Source/OnnxRuntime/OnnxRuntime.Build.cs\" Change line 20 and 21 as follows: Before bool doUseCuda = true; bool doUseTensorRT = true; After bool doUseCuda = false; bool doUseTensorRT = false; Build the project.","title":"Build without CUDA and TensorRT"},{"location":"tips-edit-onnx/","text":"Edit ONNX files Sometimes you need to edit an ONNX file a little. (For example, when you want to combine two ONNX files into one.) A useful technique in such cases is to convert ONNX to text format, edit it with a text editor, and then convert it back to the original format. See this page if you are interested.","title":"Edit ONNX files"},{"location":"tips-edit-onnx/#edit-onnx-files","text":"Sometimes you need to edit an ONNX file a little. (For example, when you want to combine two ONNX files into one.) A useful technique in such cases is to convert ONNX to text format, edit it with a text editor, and then convert it back to the original format. See this page if you are interested.","title":"Edit ONNX files"}]}