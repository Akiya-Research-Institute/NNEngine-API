\hypertarget{class_onnx_model}{}\doxysection{Onnx\+Model Class Reference}
\label{class_onnx_model}\index{OnnxModel@{OnnxModel}}


Manages a ONNX model  


\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_onnx_model_a2508b98cced724f617049c30644b5c3b}{Onnx\+Model}} (const FString onnx\+File\+Path, const EOnnx\+Provider onnx\+Provider, const int gpu\+Device\+Id=0)
\begin{DoxyCompactList}\small\item\em Open ONNX model \end{DoxyCompactList}\item 
\mbox{\Hypertarget{class_onnx_model_ad565f282fd7e7552063d09476e22a9a5}\label{class_onnx_model_ad565f282fd7e7552063d09476e22a9a5}} 
{\bfseries $\sim$\+Onnx\+Model} ()
\begin{DoxyCompactList}\small\item\em Close ONNX model \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_onnx_model_abc4f9b17570cb07d52ea68a01a02d81e}{bind\+Input}} (const \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} \&tensor\+Info, void $\ast$data\+Buffer)
\begin{DoxyCompactList}\small\item\em Specify a pre-\/allocated data buffer that will be used as the input to this model. The data will be interpreted as a tensor of specified information. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_onnx_model_ab4e00009de2d664ad4141da7165d8a85}{bind\+Output}} (const \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} \&tensor\+Info, void $\ast$data\+Buffer)
\begin{DoxyCompactList}\small\item\em Specify a pre-\/allocated data buffer that will be used for the output from this model. The data will be interpreted as a tensor of specified information. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{class_onnx_model_a1612ec615d739088a9b461ed7cd6fb2d}\label{class_onnx_model_a1612ec615d739088a9b461ed7cd6fb2d}} 
void {\bfseries clear\+Bound\+Inputs} ()
\begin{DoxyCompactList}\small\item\em Clear the boud inputs \end{DoxyCompactList}\item 
\mbox{\Hypertarget{class_onnx_model_ab0fe91d917dec120a51cd7375c3259fb}\label{class_onnx_model_ab0fe91d917dec120a51cd7375c3259fb}} 
void {\bfseries clear\+Bound\+Outputs} ()
\begin{DoxyCompactList}\small\item\em Clear the boud outputs \end{DoxyCompactList}\item 
\mbox{\Hypertarget{class_onnx_model_a3a2e9b1616ee1967d9238716a2d2383b}\label{class_onnx_model_a3a2e9b1616ee1967d9238716a2d2383b}} 
void {\bfseries run} ()
\begin{DoxyCompactList}\small\item\em Execute the model. Note that you need to bind input and output before calling this method. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{class_onnx_model_a61e41b7556d8b7a9d492b755041bda74}\label{class_onnx_model_a61e41b7556d8b7a9d492b755041bda74}} 
TArray$<$ \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} $>$ {\bfseries input\+Tensors\+Info}
\begin{DoxyCompactList}\small\item\em The name, size, shape, and type of the input tensors of this model. Automatically filled by the constructor. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{class_onnx_model_abc55011b9b82388c9b79bf8966122a94}\label{class_onnx_model_abc55011b9b82388c9b79bf8966122a94}} 
TArray$<$ \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} $>$ {\bfseries output\+Tensors\+Info}
\begin{DoxyCompactList}\small\item\em The name, size, shape, and type of the output tensors of this model. Automatically filled by the constructor. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Manages a ONNX model 

\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{class_onnx_model_a2508b98cced724f617049c30644b5c3b}\label{class_onnx_model_a2508b98cced724f617049c30644b5c3b}} 
\index{OnnxModel@{OnnxModel}!OnnxModel@{OnnxModel}}
\index{OnnxModel@{OnnxModel}!OnnxModel@{OnnxModel}}
\doxysubsubsection{\texorpdfstring{OnnxModel()}{OnnxModel()}}
{\footnotesize\ttfamily Onnx\+Model\+::\+Onnx\+Model (\begin{DoxyParamCaption}\item[{const FString}]{onnx\+File\+Path,  }\item[{const EOnnx\+Provider}]{onnx\+Provider,  }\item[{const int}]{gpu\+Device\+Id = {\ttfamily 0} }\end{DoxyParamCaption})}



Open ONNX model 


\begin{DoxyParams}{Parameters}
{\em onnx\+File\+Path} & Path of the .onnx file \\
\hline
{\em onnx\+Provider} & Execution provider used for this model \\
\hline
{\em gpu\+Device\+Id} & Which GPU to be used \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{class_onnx_model_abc4f9b17570cb07d52ea68a01a02d81e}\label{class_onnx_model_abc4f9b17570cb07d52ea68a01a02d81e}} 
\index{OnnxModel@{OnnxModel}!bindInput@{bindInput}}
\index{bindInput@{bindInput}!OnnxModel@{OnnxModel}}
\doxysubsubsection{\texorpdfstring{bindInput()}{bindInput()}}
{\footnotesize\ttfamily void Onnx\+Model\+::bind\+Input (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} \&}]{tensor\+Info,  }\item[{void $\ast$}]{data\+Buffer }\end{DoxyParamCaption})}



Specify a pre-\/allocated data buffer that will be used as the input to this model. The data will be interpreted as a tensor of specified information. 


\begin{DoxyParams}{Parameters}
{\em tensor\+Info} & The information (name, shape, etc.) of the input tensor. For models with fixed shape, just use input\+Tensors\+Info. For models with dynamic shape, change the values of size and shape in input\+Tensors\+Info \\
\hline
{\em data\+Buffer} & Pointer for the pre-\/allocated data buffer that will be used as the input to this model \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_onnx_model_ab4e00009de2d664ad4141da7165d8a85}\label{class_onnx_model_ab4e00009de2d664ad4141da7165d8a85}} 
\index{OnnxModel@{OnnxModel}!bindOutput@{bindOutput}}
\index{bindOutput@{bindOutput}!OnnxModel@{OnnxModel}}
\doxysubsubsection{\texorpdfstring{bindOutput()}{bindOutput()}}
{\footnotesize\ttfamily void Onnx\+Model\+::bind\+Output (\begin{DoxyParamCaption}\item[{const \mbox{\hyperlink{struct_f_onnx_tensor_info}{FOnnx\+Tensor\+Info}} \&}]{tensor\+Info,  }\item[{void $\ast$}]{data\+Buffer }\end{DoxyParamCaption})}



Specify a pre-\/allocated data buffer that will be used for the output from this model. The data will be interpreted as a tensor of specified information. 


\begin{DoxyParams}{Parameters}
{\em tensor\+Info} & The information (name, shape, etc.) of the output tensor. For models with fixed shape, just use output\+Tensors\+Info. For models with dynamic shape, change the values of size and shape in output\+Tensors\+Info \\
\hline
{\em data\+Buffer} & Pointer for the pre-\/allocated data buffer that will be used for the output from this model \\
\hline
\end{DoxyParams}
